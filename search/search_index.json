{"config":{"lang":["en"],"separator":"[\\\\s\\\\-]+","pipeline":["stemmer","stopWordFilter"]},"docs":[{"location":"","title":"Knowledge Builder Docs","text":"<p>This project is WIP, contributors are welcome.</p>"},{"location":"#high-level-flow","title":"High-Level Flow","text":"<pre><code>sequenceDiagram\n    autonumber\n    participant DP as DocumentProcessor\n    participant AO as AgentOrchestrator\n    participant LLM as ReAct Agent (Ollama)\n    participant MCP as Neo4j Memory MCP\n    participant N4J as Neo4j\n\n    DP-&gt;&gt;AO: process_chunk(text_chunk)\n    AO-&gt;&gt;AO: Stage 1: Primary prompt &amp; timeout\n    AO-&gt;&gt;LLM: Run ReAct agent\n    LLM--&gt;&gt;AO: Structured / freeform output\n    AO-&gt;&gt;AO: Parse tool calls\n    AO-&gt;&gt;AO: Stage 2: Execute AI tool calls\n    loop For each parsed call\n        AO-&gt;&gt;MCP: invoke_tool(name, params)\n        MCP-&gt;&gt;N4J: Graph mutation / query\n        N4J--&gt;&gt;MCP: Result\n        MCP--&gt;&gt;AO: Tool result\n    end\n    AO-&gt;&gt;AO: Sufficiency check\n    alt AI output insufficient\n        AO-&gt;&gt;AO: Stage 3: Heuristic augmentation\n        AO-&gt;&gt;AO: Extract entities from text\n        AO-&gt;&gt;MCP: create_entities / add_observations\n        MCP-&gt;&gt;N4J: Writes\n    end\n    alt Still insufficient\n        AO-&gt;&gt;AO: Stage 4: Forced minimal write\n        AO-&gt;&gt;MCP: Ensure Evidence + GenericTopic\n    end\n    AO-&gt;&gt;AO: Stage 5: Finalize &amp; summarize\n    AO--&gt;&gt;DP: Executed tool call list</code></pre>"},{"location":"#key-guarantees","title":"Key Guarantees","text":"<ul> <li>Deterministic Evidence node per chunk</li> <li>Progressive enrichment ladder (AI \u2192 heuristic \u2192 forced)</li> <li>Bounded agent time (single timeout wrapper)</li> <li>Centralized logging &amp; tool call execution</li> </ul>"},{"location":"#more-details","title":"More Details","text":"<p>See the full design in <code>agent_orchestrator.md</code> .</p>"},{"location":"agent_orchestrator/","title":"Agent Orchestrator Design (Refactored, Modular)","text":"<p>Last updated: 2025-10-12</p>"},{"location":"agent_orchestrator/#overview","title":"Overview","text":"<p>Structured ingestion of document chunks into the Neo4j knowledge graph is coordinated by a set of focused modules instead of a single monolith:</p> <ul> <li><code>agent_orchestrator.py</code> \u2013 Staged control flow, evidence linking, logging, public thin wrappers</li> <li><code>agent_execution.py</code> \u2013 Async agent invocation, timeout handling, message parsing, suggested tool execution ordering</li> <li><code>fallback_strategies.py</code> \u2013 Heuristic + forced\u2011minimal entity/observation generation</li> <li><code>agent_prompts.py</code> \u2013 Prompt template constant</li> <li><code>json_utils.py</code> (+ <code>ToolCallExtractor</code>) \u2013 High\u2011resilience JSON / tool call extraction</li> </ul> <p>Core objectives remain: * Bounded per\u2011chunk runtime (timeouts + ordered fallbacks) * High observability (uniform stage logs, truncated previews) * Deterministic evidence nodes for traceability * Progressive enhancement: Agent \u2192 AI\u2011suggested calls \u2192 Generated heuristics \u2192 Forced minimal * Testability via slim public wrappers and separable modules (SRP &amp; SOLID friendly)</p>"},{"location":"agent_orchestrator/#processing-stages-in-agentorchestratorprocess_chunk","title":"Processing Stages (in <code>AgentOrchestrator.process_chunk</code> )","text":"<ol> <li>Primary (<code>_stage_primary_prompt</code>) \u2013 Build prompt, invoke async ReAct agent (<code>run_agent_with_timeout</code>) and log last AI message.</li> <li>Suggested (<code>_stage_execute_ai_suggestions</code>) \u2013 If no writes yet, parse inline JSON tool call blocks (via <code>ToolCallExtractor</code>), execute them.</li> <li>Generate (<code>_stage_generate_calls_from_content</code>) \u2013 If still no writes, derive entities from AI content or raw text heuristics (delegates to <code>fallback_strategies.generate_tool_calls_from_content</code>).</li> <li>Forced (<code>_stage_force_minimal</code>) \u2013 Guarantee a minimal semantic footprint (evidence + 1\u2013N simple entities + observations) using <code>force_minimal_write</code>.</li> <li>Finalize (<code>_stage_finalize</code>) \u2013 De\u2011duplicate touched entities, create evidence (if not already), link them, and retry deferred relations (deduped).</li> </ol> <p>Only the minimal necessary stages run: once a stage achieves a successful write ( <code>wrote=True</code> ), subsequent enrichment stages are skipped except Finalize.</p>"},{"location":"agent_orchestrator/#timeout-execution-model","title":"Timeout Execution Model","text":"<p>Two layers exist: * <code>agent_execution.run_agent_with_timeout</code> \u2013 The canonical implementation: <code>asyncio.run</code> + <code>asyncio.wait_for</code>, robust loop fallback and rich error logging. * <code>AgentOrchestrator._run_with_timeout</code> \u2013 Thin backward\u2011compatibility shim retained for tests that monkey\u2011patch a <code>run_agent</code> coroutine. It delegates to the shared implementation unless a patched async <code>run_agent</code> attribute is present.</p> <p>Timeouts yield an empty message list, triggering heuristic progression instead of bubbling exceptions.</p>"},{"location":"agent_orchestrator/#helper-abstractions-current-canonical-set","title":"Helper Abstractions (Current Canonical Set)","text":"Purpose Helper Notes Evidence naming <code>_make_evidence_name(source_id, chunk_id)</code> Format: <code>Evidence &lt;SOURCE_ID&gt;-&lt;CHUNK_ID&gt;</code> Evidence payload <code>_build_evidence_entity(source_id, chunk_id, source_url)</code> Adds source metadata in observations list Tool invocation <code>_invoke_tool(name, params)</code> Central logging + success detection Heuristic entity candidates <code>fallback_strategies.collect_capitalized_entities</code> Moved out of orchestrator (was previously internal) Evidence linking <code>_link_entities_to_evidence(evidence_name, entity_names)</code> Uses <code>create_relations</code> tool Context observation <code>_add_context_observation(entity_name, text, prefix)</code> Truncates to configured limit Touched name extraction <code>_collect_touched_names(tool_name, params)</code> Normalizes names from tool arguments Relation retry normalization <code>_normalize_relation_for_retry</code> / <code>_relation_key</code> Deduplicates before final retry <p>Public wrappers ( <code>make_evidence_name</code> , <code>build_evidence_entity</code> , <code>collect_capitalized_entities</code> , <code>invoke_tool_safe</code> , <code>add_context_observation</code> ) keep tests stable without exposing internal invariants directly.</p> <p>Deprecated / Removed: * <code>_collect_capitalized_entities</code> (old private) \u2013 superseded by exported function in <code>fallback_strategies</code>. * Large inline JSON parsing helpers \u2013 replaced by consolidated <code>json_utils</code> + <code>ToolCallExtractor</code> resilient strategy.</p>"},{"location":"agent_orchestrator/#fallback-strategy-resilience-ladder","title":"Fallback Strategy (Resilience Ladder)","text":"<ol> <li>Agent Tool Calls \u2013 Ideal rich graph updates.</li> <li>Suggested Call Execution \u2013 Executes tool JSON the agent embedded in its last message.</li> <li>Generated Calls From Content \u2013 Derive entities/relations heuristically from AI content + raw text.</li> <li>Forced Minimal \u2013 Ensure at least: Evidence node, 1\u2013N lightweight entities, observations, and evidence relations.</li> </ol> <p>This guarantees every processed chunk produces: * A deterministic Evidence node (<code>Evidence &lt;SOURCE_ID&gt;-&lt;CHUNK_ID&gt;</code>) * At least one semantic anchor (entity name and/or observation) * Relations tying entities to the evidence for future enrichment</p>"},{"location":"agent_orchestrator/#ladder-diagram","title":"Ladder Diagram","text":"<pre><code>flowchart LR\n   A[Start Chunk] --&gt; B{Agent Messages}\n   B --&gt;|Rich / Tool Writes| H[Finalize]\n   B --&gt;|No Writes| C[Suggested JSON Execution]\n   C --&gt;|Writes| H\n   C --&gt;|Still None| D[Generate From Content]\n   D --&gt;|Writes| H\n   D --&gt;|None| E[Forced Minimal]\n   E --&gt; H\n   H --&gt; I[Evidence + Entities Linked]</code></pre>"},{"location":"agent_orchestrator/#logging-observability","title":"Logging &amp; Observability","text":"<ul> <li>Consistent stage prefix formatting via <code>_log_stage</code> / <code>_log_stage_warn</code>.</li> <li>Last AI message content always logged (truncated) for transparency.</li> <li>Every tool call funnels through <code>_invoke_tool</code> (central intent/result logging) or <code>agent_execution.invoke_and_collect</code> (suggested calls path).</li> <li>Heuristic / forced paths emit explicit <code>[agent] generated</code> / <code>forced</code> prefixes to distinguish origin.</li> </ul>"},{"location":"agent_orchestrator/#exception-handling","title":"Exception Handling","text":"<ul> <li>Narrow, explicit exception classes where predictable (argument/type/timeout).</li> <li>Broad catch only at <code>process_chunk</code> boundary to trigger emergency forced minimal fallback without silent loss.</li> <li>Relation retry failures are warnings, not fatal.</li> </ul>"},{"location":"agent_orchestrator/#testability","title":"Testability","text":"<p>Tests rely on: * Presence of <code>_run_with_timeout</code> symbol for monkey\u2011patching. * Deterministic evidence naming scheme. * Stable ordering of tool execution (observations \u2192 entities \u2192 other \u2192 relations) enforced in <code>agent_execution.order_tool_calls</code>. * Heuristic functions exported from <code>fallback_strategies</code> for direct unit coverage.</p>"},{"location":"agent_orchestrator/#data-naming-conventions-current","title":"Data &amp; Naming Conventions (Current)","text":"<ul> <li>Evidence node name: <code>Evidence &lt;SOURCE_ID&gt;-&lt;CHUNK_ID&gt;</code></li> <li>Relation keys use active voice per Neo4j Memory MCP expectations.</li> <li>Observation payloads truncated using <code>Config.LOG_TOOL_PREVIEW_MAX</code> / local helper limits to prevent bloat.</li> </ul>"},{"location":"agent_orchestrator/#extensibility-guidance","title":"Extensibility Guidance","text":"<ol> <li>Put new async agent mechanics in <code>agent_execution.py</code>.</li> <li>Keep heuristic-only logic in <code>fallback_strategies.py</code>.</li> <li>Add a new stage only if it introduces a distinct decision boundary; otherwise enrich an existing stage.</li> <li>Reuse <code>_invoke_tool</code> instead of calling the connector directly\u2014preserves uniform logging &amp; success detection.</li> <li>Update docs + ADR when adding or changing a stage.</li> </ol>"},{"location":"agent_orchestrator/#performance-notes","title":"Performance Notes","text":"<ul> <li>Early exit after first successful write prevents unnecessary heuristic passes.</li> <li>Lightweight regex heuristics avoid secondary LLM calls.</li> <li>Deduped relation retry prevents redundant graph edges.</li> </ul>"},{"location":"agent_orchestrator/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>Adaptive timeouts by chunk size or historical agent latency.</li> <li>Prometheus metrics for stage durations &amp; fallback counts.</li> <li>Cross\u2011chunk entity consolidation / alias resolution.</li> <li>Re\u2011enrichment pipeline targeting minimally populated evidence nodes.</li> </ul>"},{"location":"agent_orchestrator/#summary","title":"Summary","text":"<p>The orchestrator is now a modular, low\u2011complexity pipeline: clear stage boundaries, centralized logging, deterministic fallbacks, and isolated heuristics. This architecture reduces risk, simplifies testing, and prepares the system for incremental capability upgrades without re\u2011introducing monolith debt.</p>"},{"location":"root-readme/","title":"Repository Overview","text":"<p>This page provides a high-level overview of the repository. For full details, see the root <code>README.md</code> on GitHub.</p> <p>NOTE: This copy is intentionally minimal to avoid duplication and conflicts with <code>index.md</code> .</p>"},{"location":"scheduler/","title":"Scheduler &amp; Run Coordination","text":"<p>The scheduler ensures periodic, non-overlapping ingestion runs with graceful shutdown semantics.</p>"},{"location":"scheduler/#responsibilities","title":"Responsibilities","text":"<ul> <li>Periodic trigger (default every 5 minutes via <code>schedule</code> library)</li> <li>Overlap prevention via in-process run lock</li> <li>Early exit if stop event set (signal handler)</li> <li>Delegation to Document Processor for actual ETL work</li> </ul>"},{"location":"scheduler/#sequence","title":"Sequence","text":"<pre><code>sequenceDiagram\n    autonumber\n    participant Main as main.py\n    participant S as Scheduler\n    participant DP as DocumentProcessor\n    participant SM as StateManager\n\n    Main-&gt;&gt;S: initialize(schedule_time)\n    loop Every interval\n        S-&gt;&gt;S: is_running?\n        alt Already Running\n            S--&gt;&gt;S: Log skip\n        else Not Running\n            S-&gt;&gt;S: acquire lock\n            S-&gt;&gt;DP: run_import()\n            DP-&gt;&gt;SM: load_state()\n            DP-&gt;&gt;DP: fetch_documents()\n            DP-&gt;&gt;DP: process_documents()\n            DP-&gt;&gt;SM: update_last_id/hash\n            DP--&gt;&gt;S: run complete\n            S-&gt;&gt;S: release lock\n        end\n    end</code></pre>"},{"location":"scheduler/#locking-strategy","title":"Locking Strategy","text":"<ul> <li>Simple in-memory boolean / threading. Lock (sufficient for single-process container)</li> <li>Prevents scheduler drift causing overlapping long-running runs</li> </ul>"},{"location":"scheduler/#shutdown","title":"Shutdown","text":"<ul> <li>Signal handler sets a stop event</li> <li>Scheduler loop checks stop flag before starting a new cycle</li> <li>In-flight document/chunk completes before process exits</li> </ul>"},{"location":"scheduler/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>External distributed lock (e.g., Redis) for horizontal scaling</li> <li>Metrics around run duration &amp; skipped intervals</li> <li>Backoff / jitter to avoid thundering herd if scaled</li> </ul>"},{"location":"adr/","title":"Architecture Decision Records","text":"<p>This index lists all ADRs in the project.</p>"},{"location":"adr/#adrs","title":"ADRs","text":"<ul> <li>0001 Orchestrator Refactor</li> </ul>"},{"location":"adr/0001-orchestrator-refactor/","title":"ADR 0001: Staged Agent Orchestrator Refactor","text":"<p>Date: 2025-10-11 (updated 2025-10-12) Status: Accepted</p>"},{"location":"adr/0001-orchestrator-refactor/#context","title":"Context","text":"<p>The initial orchestrator was a single oversized file containing: agent invocation, JSON/tool call parsing, heuristic fallbacks, evidence creation, and logging patterns. This violated SRP, made unit tests brittle, and obscured fallback guarantees. Type + lint complexity increased as one method accumulated timeout handling, parsing branches, and emergency recovery logic.</p>"},{"location":"adr/0001-orchestrator-refactor/#decision","title":"Decision","text":"<p>Adopt a modular, staged pipeline split across dedicated modules:</p> <ol> <li>Primary Agent Run (timeout bound)</li> <li>Execute Parsed Tool Calls (suggested JSON inside last AI message)</li> <li>Heuristic Generation (derive tool calls from AI content + raw text)</li> <li>Forced Minimal Write (guaranteed baseline evidence + entities + observations)</li> <li>Finalize (evidence linking + relation retry)</li> </ol> <p>Module allocation: * <code>agent_orchestrator.py</code> \u2013 Stage orchestration, evidence utilities, logging, relation retry coordination * <code>agent_execution.py</code> \u2013 Async agent call (<code>run_agent_async</code>), timeout wrapper (<code>run_agent_with_timeout</code>), message processing, suggested tool ordering/execution * <code>fallback_strategies.py</code> \u2013 Heuristic entity mining, forced minimal write, generated call synthesis * <code>agent_prompts.py</code> \u2013 Prompt template constant * <code>json_utils.py</code> / <code>ToolCallExtractor</code> \u2013 Resilient JSON extraction &amp; normalization</p> <p>Public thin wrappers maintain test stability (evidence naming, tool invocation, heuristic entity collection) while internals can evolve.</p>"},{"location":"adr/0001-orchestrator-refactor/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Incremental clean-up inside monolith \u2013 Rejected: risk of regression remained high; complexity ceiling not reduced.</li> <li>Introduce a planning DSL \u2013 Over-engineered for current scale; added learning curve.</li> <li>Pure heuristic approach (remove LLM) \u2013 Loss of adaptive extraction and semantic richness.</li> </ul>"},{"location":"adr/0001-orchestrator-refactor/#consequences","title":"Consequences","text":"<p>Positive: * Functions stay under targeted length &amp; cognitive complexity thresholds. * Clear extension seams (add/modify stage vs. expanding a giant method). * Improved test isolation (each helper/module directly testable). * Deterministic fallback ladder ensures no silent no-op chunks.</p> <p>Trade-offs / Neutral: * Slight overhead from additional module boundaries. * Documentation must track multiple files (mitigated via this ADR + updated design doc).</p>"},{"location":"adr/0001-orchestrator-refactor/#implementation-notes-updated","title":"Implementation Notes (Updated)","text":"<ul> <li><code>_run_with_timeout</code> inside orchestrator is now a thin backward-compatibility shim; canonical timeout logic lives in <code>agent_execution.run_agent_with_timeout</code>.</li> <li>Helper <code>_collect_capitalized_entities</code> removed; replaced by <code>fallback_strategies.collect_capitalized_entities</code> (exported for reuse/tests).</li> <li>Legacy ad-hoc JSON parsing superseded by consolidated <code>ToolCallExtractor</code> using <code>json_utils</code> (orjson-based) for speed + resilience.</li> <li>Tool execution ordering relocated to <code>agent_execution.order_tool_calls</code> to preserve expected sequence (observations \u2192 entities \u2192 other \u2192 relations).</li> <li>Relation retry dedup logic extracted via <code>_normalize_relation_for_retry</code> + <code>_relation_key</code> helpers.</li> </ul>"},{"location":"adr/0001-orchestrator-refactor/#metrics-validation","title":"Metrics / Validation","text":"<ul> <li>Unit tests cover timeout wrapper presence (monkeypatch compatibility) and heuristic fallbacks.</li> <li>Manual and test logs confirm: stage banners, truncated AI output, clear origin prefixes (\"generated\", \"forced\").</li> <li>Line count reduction in orchestrator keeps it below monolith guard threshold (guard script external, not part of ADR scope).</li> </ul>"},{"location":"adr/0001-orchestrator-refactor/#future-work","title":"Future Work","text":"<ul> <li>Adaptive timeout heuristics (chunk length, historical agent latency).</li> <li>Metrics export (Prometheus) for stage durations, fallback frequency, retry counts.</li> <li>Re-enrichment job to revisit minimal chunks with improved models.</li> <li>Cross-chunk entity consolidation / alias resolution pipeline.</li> </ul>"},{"location":"adr/0001-orchestrator-refactor/#status","title":"Status","text":"<p>Implemented and documented (see updated <code>docs/agent_orchestrator.md</code> ).</p>"},{"location":"api/config/","title":"Config API","text":"<p>configuration env settings environment variables config loader</p>"},{"location":"api/config/#config.Config","title":"<code>Config</code>","text":"<p>Centralized configuration management for the importer application.</p>"},{"location":"api/config/#config.Config.initialize_directories","title":"<code>initialize_directories()</code>  <code>classmethod</code>","text":"<p>Initialize required directories.</p> Source code in <code>importer/src/config.py</code> <pre><code>@classmethod\ndef initialize_directories(cls):\n    \"\"\"Initialize required directories.\"\"\"\n    cls.VAULT_DIR.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/connectors/","title":"Connectors API","text":"<p>neo4j memory mcp connector paperless api client graph database</p>"},{"location":"api/connectors/#connectors.neo4j_connector.Neo4jMemoryConnector","title":"<code>Neo4jMemoryConnector()</code>","text":"<p>Manages connection to Neo4j memory via MCP adapters.</p> Source code in <code>importer/src/connectors/neo4j_connector.py</code> <pre><code>def __init__(self):\n    self._tools_cache: List[BaseTool] = []\n    self._tools_loaded: bool = False\n</code></pre>"},{"location":"api/connectors/#connectors.neo4j_connector.Neo4jMemoryConnector.neo4j_host_port_from_url","title":"<code>neo4j_host_port_from_url(url)</code>  <code>staticmethod</code>","text":"<p>Extract host and port from Neo4j URL.</p> Source code in <code>importer/src/connectors/neo4j_connector.py</code> <pre><code>@staticmethod\ndef neo4j_host_port_from_url(url: str) -&gt; Optional[tuple[str, int]]:\n    \"\"\"Extract host and port from Neo4j URL.\"\"\"\n    with contextlib.suppress(Exception):\n        p = urlparse(url)\n        if p.hostname:\n            return p.hostname, p.port or 7687\n    return None\n</code></pre>"},{"location":"api/connectors/#connectors.neo4j_connector.Neo4jMemoryConnector.build_stdio_server_config","title":"<code>build_stdio_server_config()</code>  <code>staticmethod</code>","text":"<p>Build MCP stdio server configuration.</p> Source code in <code>importer/src/connectors/neo4j_connector.py</code> <pre><code>@staticmethod\ndef build_stdio_server_config() -&gt; Dict[str, Any]:\n    \"\"\"Build MCP stdio server configuration.\"\"\"\n    base_cmd = Config.MEMORY_MCP_CMD or Config.DEFAULT_MCP_CMD\n    db_url = Config.NEO4J_URL or f\"bolt://{Config.NEO4J_HOST}:{Config.NEO4J_PORT}\"\n    args: List[str] = [\"--db-url\", db_url]\n\n    if Config.NEO4J_USER_ENV:\n        args += [\"--username\", Config.NEO4J_USER_ENV]\n    if Config.NEO4J_PASS_ENV:\n        args += [\"--password\", Config.NEO4J_PASS_ENV]\n    if Config.NEO4J_DATABASE:\n        args += [\"--database\", Config.NEO4J_DATABASE]\n\n    logger.info(f\"[mcp] stdio cmd: {base_cmd} args={args}\")\n    return {\n        \"neo4j\": {\n            \"command\": base_cmd,\n            \"args\": args,\n            \"transport\": \"stdio\",\n        }\n    }\n</code></pre>"},{"location":"api/connectors/#connectors.neo4j_connector.Neo4jMemoryConnector.load_mcp_tools","title":"<code>load_mcp_tools()</code>  <code>async</code> <code>classmethod</code>","text":"<p>Load MCP tools from configuration.</p> Source code in <code>importer/src/connectors/neo4j_connector.py</code> <pre><code>@classmethod\nasync def load_mcp_tools(cls) -&gt; List[BaseTool]:\n    \"\"\"Load MCP tools from configuration.\"\"\"\n    config = cls.build_stdio_server_config()\n    client = MultiServerMCPClient(config)\n    tools = await client.get_tools()\n    logger.info(f\"[mcp] loaded tools via adapter: {[t.name for t in tools]}\")\n\n    # Debug: log tool schemas\n    for tool in tools:\n        try:\n            schema = None\n            args_schema = getattr(tool, \"args_schema\", None)\n            if args_schema is not None:\n                if hasattr(args_schema, \"model_json_schema\"):\n                    schema = args_schema.model_json_schema()\n                elif hasattr(args_schema, \"schema\"):\n                    schema = args_schema.schema()\n            if isinstance(schema, dict):\n                required = schema.get(\"required\") or []\n                logger.info(\n                    f\"[mcp] tool schema name={tool.name} required={required}\"\n                )\n        except Exception:\n            logger.debug(f\"[mcp] tool schema dump failed for {tool.name}\")\n\n    return tools\n</code></pre>"},{"location":"api/connectors/#connectors.neo4j_connector.Neo4jMemoryConnector.ensure_mcp_tools","title":"<code>ensure_mcp_tools()</code>  <code>async</code>","text":"<p>Ensure MCP tools are loaded and cached.</p> Source code in <code>importer/src/connectors/neo4j_connector.py</code> <pre><code>async def ensure_mcp_tools(self) -&gt; List[BaseTool]:\n    \"\"\"Ensure MCP tools are loaded and cached.\"\"\"\n    if not self._tools_loaded:\n        self._tools_cache = await self.load_mcp_tools()\n        self._tools_loaded = True\n    return self._tools_cache\n</code></pre>"},{"location":"api/connectors/#connectors.neo4j_connector.Neo4jMemoryConnector.get_tools_by_name_sync","title":"<code>get_tools_by_name_sync()</code>","text":"<p>Get the tool dictionary by name (synchronous).</p> Source code in <code>importer/src/connectors/neo4j_connector.py</code> <pre><code>def get_tools_by_name_sync(self) -&gt; Dict[str, BaseTool]:\n    \"\"\"Get the tool dictionary by name (synchronous).\"\"\"\n    try:\n        tools = asyncio.run(self.ensure_mcp_tools())\n    except RuntimeError:\n        tools = self._tools_cache\n    return {t.name: t for t in tools}\n</code></pre>"},{"location":"api/connectors/#connectors.neo4j_connector.Neo4jMemoryConnector.try_async_invoke","title":"<code>try_async_invoke(tool, params)</code>  <code>staticmethod</code>","text":"<p>Try to invoke the tool using the async ainvoke method.</p> Source code in <code>importer/src/connectors/neo4j_connector.py</code> <pre><code>@staticmethod\ndef try_async_invoke(tool: BaseTool, params: Dict[str, Any]) -&gt; Any:\n    \"\"\"Try to invoke the tool using the async ainvoke method.\"\"\"\n    if not hasattr(tool, \"ainvoke\"):\n        return None\n\n    try:\n        return asyncio.run(tool.ainvoke(params))\n    except Exception:\n        return None\n</code></pre>"},{"location":"api/connectors/#connectors.neo4j_connector.Neo4jMemoryConnector.try_sync_invoke","title":"<code>try_sync_invoke(tool, params)</code>  <code>staticmethod</code>","text":"<p>Try to invoke the tool using the sync invoke method.</p> Source code in <code>importer/src/connectors/neo4j_connector.py</code> <pre><code>@staticmethod\ndef try_sync_invoke(tool: BaseTool, params: Dict[str, Any]) -&gt; Any:\n    \"\"\"Try to invoke the tool using the sync invoke method.\"\"\"\n    if not hasattr(tool, \"invoke\"):\n        return None\n\n    try:\n        return tool.invoke(params)\n    except Exception as exc:\n        if \"async\" in str(exc).lower() or \"await\" in str(exc).lower():\n            return Neo4jMemoryConnector.try_async_invoke(tool, params)\n        return None\n</code></pre>"},{"location":"api/connectors/#connectors.neo4j_connector.Neo4jMemoryConnector.try_legacy_run","title":"<code>try_legacy_run(tool, params)</code>  <code>staticmethod</code>","text":"<p>Try to invoke the tool using the legacy run method.</p> Source code in <code>importer/src/connectors/neo4j_connector.py</code> <pre><code>@staticmethod\ndef try_legacy_run(tool: BaseTool, params: Dict[str, Any]) -&gt; Any:\n    \"\"\"Try to invoke the tool using the legacy run method.\"\"\"\n    if not hasattr(tool, \"run\"):\n        return None\n\n    try:\n        return tool.run(**params)\n    except Exception:\n        return None\n</code></pre>"},{"location":"api/connectors/#connectors.neo4j_connector.Neo4jMemoryConnector.try_callable_fallback","title":"<code>try_callable_fallback(tool, params)</code>  <code>staticmethod</code>","text":"<p>Try to invoke the tool as a callable.</p> Source code in <code>importer/src/connectors/neo4j_connector.py</code> <pre><code>@staticmethod\ndef try_callable_fallback(tool: BaseTool, params: Dict[str, Any]) -&gt; Any:\n    \"\"\"Try to invoke the tool as a callable.\"\"\"\n    if not callable(tool):\n        raise RuntimeError(f\"Tool '{tool}' cannot be invoked with any known method\")\n\n    try:\n        return tool(**params)\n    except Exception as exc:\n        raise RuntimeError(f\"Failed to invoke tool '{tool}': {exc}\") from exc\n</code></pre>"},{"location":"api/connectors/#connectors.neo4j_connector.Neo4jMemoryConnector.try_tool_invocation_methods","title":"<code>try_tool_invocation_methods(tool, params)</code>  <code>classmethod</code>","text":"<p>Try different invocation methods for a tool in order of preference.</p> Source code in <code>importer/src/connectors/neo4j_connector.py</code> <pre><code>@classmethod\ndef try_tool_invocation_methods(cls, tool: BaseTool, params: Dict[str, Any]) -&gt; Any:\n    \"\"\"Try different invocation methods for a tool in order of preference.\"\"\"\n    result = cls.try_async_invoke(tool, params)\n    if result is not None:\n        return result\n\n    result = cls.try_sync_invoke(tool, params)\n    if result is not None:\n        return result\n\n    result = cls.try_legacy_run(tool, params)\n    return result if result is not None else cls.try_callable_fallback(tool, params)\n</code></pre>"},{"location":"api/connectors/#connectors.neo4j_connector.Neo4jMemoryConnector.invoke_tool_by_name","title":"<code>invoke_tool_by_name(name, params)</code>","text":"<p>Invoke a tool by name with the given parameters.</p> Source code in <code>importer/src/connectors/neo4j_connector.py</code> <pre><code>def invoke_tool_by_name(self, name: str, params: Dict[str, Any]) -&gt; Any:\n    \"\"\"Invoke a tool by name with the given parameters.\"\"\"\n    tools = self.get_tools_by_name_sync()\n    if name not in tools:\n        raise ValueError(f\"Tool '{name}' not found\")\n\n    tool = tools[name]\n    normalized_params = ToolCallNormalizer.normalize_params(name, params)\n    return self.try_tool_invocation_methods(tool, normalized_params)\n</code></pre>"},{"location":"api/connectors/#connectors.paperless_connector.PaperlessConnector","title":"<code>PaperlessConnector</code>","text":"<p>Handles document retrieval and processing from Paperless.</p>"},{"location":"api/connectors/#connectors.paperless_connector.PaperlessConnector.wait_for_token","title":"<code>wait_for_token(timeout_seconds=0)</code>  <code>staticmethod</code>","text":"<p>Wait for a Paperless authentication token.</p> Source code in <code>importer/src/connectors/paperless_connector.py</code> <pre><code>@staticmethod\ndef wait_for_token(timeout_seconds: int = 0) -&gt; str:\n    \"\"\"Wait for a Paperless authentication token.\"\"\"\n    if Config.PAPERLESS_TOKEN:\n        return Config.PAPERLESS_TOKEN\n\n    token_path = Config.PAPERLESS_TOKEN_FILE\n    if not token_path:\n        raise RuntimeError(\n            \"Neither PAPERLESS_TOKEN nor PAPERLESS_TOKEN_FILE is specified.\"\n        )\n\n    logger.info(f\"[bootstrap] Watching token: {token_path}\")\n    deadline = (time.time() + timeout_seconds) if timeout_seconds &gt; 0 else None\n\n    while True:\n        with contextlib.suppress(Exception):\n            if os.path.isfile(token_path) and os.path.getsize(token_path) &gt; 0:\n                content = Path(token_path).read_text(encoding=\"utf-8\").strip()\n                if content and content.upper() != \"PENDING\":\n                    logger.info(\"[bootstrap] Paperless token read.\")\n                    return content\n\n        if deadline and time.time() &gt; deadline:\n            raise RuntimeError(\"Token not available within the specified time.\")\n        time.sleep(2)\n</code></pre>"},{"location":"api/connectors/#connectors.paperless_connector.PaperlessConnector.get_headers","title":"<code>get_headers()</code>  <code>classmethod</code>","text":"<p>Get authorization headers for Paperless API.</p> Source code in <code>importer/src/connectors/paperless_connector.py</code> <pre><code>@classmethod\ndef get_headers(cls) -&gt; Dict[str, str]:\n    \"\"\"Get authorization headers for Paperless API.\"\"\"\n    token = cls.wait_for_token()\n    return {\"Authorization\": f\"Token {token}\"}\n</code></pre>"},{"location":"api/connectors/#connectors.paperless_connector.PaperlessConnector.iter_documents","title":"<code>iter_documents()</code>  <code>classmethod</code>","text":"<p>Iterate through all documents from Paperless API.</p> Source code in <code>importer/src/connectors/paperless_connector.py</code> <pre><code>@classmethod\ndef iter_documents(cls) -&gt; Iterator[Dict]:\n    \"\"\"Iterate through all documents from Paperless API.\"\"\"\n    url = f\"{Config.PAPERLESS_URL}/api/documents/?ordering=id\"\n    headers = cls.get_headers()\n\n    while url:\n        with httpx.Client(timeout=30) as client:\n            response = client.get(url, headers=headers)\n            response.raise_for_status()\n            data = response.json()\n            yield from data.get(\"results\", [])\n            url = data.get(\"next\")\n</code></pre>"},{"location":"api/connectors/#connectors.paperless_connector.PaperlessConnector.get_document","title":"<code>get_document(doc_id)</code>  <code>classmethod</code>","text":"<p>Get detailed document data by ID.</p> Source code in <code>importer/src/connectors/paperless_connector.py</code> <pre><code>@classmethod\ndef get_document(cls, doc_id: int) -&gt; Dict:\n    \"\"\"Get detailed document data by ID.\"\"\"\n    url = f\"{Config.PAPERLESS_URL}/api/documents/{doc_id}/\"\n    headers = cls.get_headers()\n\n    with httpx.Client(timeout=30) as client:\n        response = client.get(url, headers=headers)\n        response.raise_for_status()\n        return response.json()\n</code></pre>"},{"location":"api/connectors/#connectors.paperless_connector.PaperlessConnector.extract_text","title":"<code>extract_text(document)</code>  <code>staticmethod</code>","text":"<p>Extract text content from document.</p> Source code in <code>importer/src/connectors/paperless_connector.py</code> <pre><code>@staticmethod\ndef extract_text(document: Dict) -&gt; str:\n    \"\"\"Extract text content from document.\"\"\"\n    return (document.get(\"content\") or \"\").strip()\n</code></pre>"},{"location":"api/connectors/#connectors.paperless_connector.PaperlessConnector.write_obsidian_note","title":"<code>write_obsidian_note(document, chunk_index, text)</code>  <code>staticmethod</code>","text":"<p>Write chunk to Obsidian vault (optional).</p> Source code in <code>importer/src/connectors/paperless_connector.py</code> <pre><code>@staticmethod\ndef write_obsidian_note(document: Dict, chunk_index: int, text: str):\n    \"\"\"Write chunk to Obsidian vault (optional).\"\"\"\n    try:\n        slug = f\"{document['id']}_c{chunk_index + 1}\"\n        meta = {\n            \"title\": document.get(\"title\") or slug,\n            \"created\": document.get(\"created\"),\n            \"source\": document.get(\"download_url\"),\n            \"paperless_id\": document[\"id\"],\n            \"chunk\": chunk_index + 1,\n        }\n        body = (\n            \"---\\n\"\n            + json.dumps(meta, ensure_ascii=False, indent=2)\n            + \"\\n---\\n\\n\"\n            + text\n            + \"\\n\"\n        )\n        (Config.VAULT_DIR / f\"{slug}.md\").write_text(body, encoding=\"utf-8\")\n        logger.info(f\"[obsidian] wrote: {slug}.md ({len(text)} chars)\")\n    except Exception:\n        logger.exception(\"Obsidian write error\")\n</code></pre>"},{"location":"api/models/","title":"Models API","text":"<p>pydantic models validation schema data model entities relations observations</p>"},{"location":"api/models/#models.DocumentWork","title":"<code>DocumentWork</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data structure representing a document processing work unit.</p>"},{"location":"api/processing/","title":"Processing API","text":"<p>processing pipeline orchestrator chunk ingestion fallback heuristic agent timeout</p> <pre><code>  show_docstring_examples: true\n</code></pre>"},{"location":"api/processing/#processing.document_processor.DocumentProcessor","title":"<code>DocumentProcessor()</code>","text":"<p>Main ETL pipeline for processing documents.</p> Source code in <code>importer/src/processing/document_processor.py</code> <pre><code>def __init__(self):\n    self.neo4j_connector = Neo4jMemoryConnector()\n    self.agent_orchestrator = AgentOrchestrator(self.neo4j_connector)\n    self.paperless_connector = PaperlessConnector()\n    self.state_manager = StateManager()\n    self.bootstrapper = ServiceBootstrapper()\n</code></pre>"},{"location":"api/processing/#processing.document_processor.DocumentProcessor.prepare_document_work","title":"<code>prepare_document_work(document)</code>","text":"<p>Prepare a document work unit with validation and chunking.</p> Source code in <code>importer/src/processing/document_processor.py</code> <pre><code>def prepare_document_work(self, document: dict) -&gt; Optional[DocumentWork]:\n    \"\"\"Prepare a document work unit with validation and chunking.\"\"\"\n    try:\n        doc_id = int(document.get(\"id\", 0))\n    except Exception:\n        return None\n\n    logger.info(f\"[doc] consider id={doc_id} force={Config.FORCE_REPROCESS}\")\n\n    if not self.state_manager.should_process_document(doc_id):\n        logger.info(\n            f\"[doc] skip id={doc_id} last_id={self.state_manager.state.get('last_id', 0)}\"\n        )\n        return None\n\n    try:\n        detailed = self.paperless_connector.get_document(doc_id)\n    except Exception as exc:\n        logger.error(f\"[doc] detail fetch failed id={doc_id}: {exc}\")\n        return None\n\n    text = self.paperless_connector.extract_text(detailed)\n\n    if not text:\n        # Try to fall back from metadata\n        fallback = \" \".join(\n            [\n                str(detailed.get(\"title\") or \"\"),\n                str(detailed.get(\"notes\") or \"\"),\n                str(detailed.get(\"original_filename\") or \"\"),\n                str(detailed.get(\"created\") or \"\"),\n            ]\n        ).strip()\n\n        if fallback:\n            logger.info(\n                f\"[doc] empty OCR content; using metadata fallback id={doc_id}\"\n            )\n        text = fallback\n\n    if not text:\n        logger.info(f\"[doc] no usable text id={doc_id}; advancing state\")\n        self.state_manager.advance_last_id(doc_id)\n        return None\n\n    text_hash = hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\n\n    if not self.state_manager.is_document_changed(doc_id, text_hash):\n        logger.info(f\"[doc] unchanged hash; skip id={doc_id}\")\n        self.state_manager.advance_last_id(doc_id)\n        return None\n\n    chunks = TextUtils.chunk_text(text)\n    logger.info(\n        f\"[doc] prepared id={doc_id} chunks={len(chunks)} first_len={len(chunks[0]) if chunks else 0}\"\n    )\n\n    source_url = str(detailed.get(\"download_url\") or \"\")\n    return DocumentWork(\n        doc_id=doc_id,\n        source_url=source_url,\n        chunks=chunks,\n        text_hash=text_hash,\n        doc=detailed,\n    )\n</code></pre>"},{"location":"api/processing/#processing.document_processor.DocumentProcessor.run_downstream_steps","title":"<code>run_downstream_steps(work)</code>","text":"<p>Execute downstream processing steps for document work.</p> Source code in <code>importer/src/processing/document_processor.py</code> <pre><code>def run_downstream_steps(self, work: DocumentWork):\n    \"\"\"Execute downstream processing steps for document work.\"\"\"\n    total_chunks = len(work.chunks)\n    successful_chunks = 0\n    failed_chunks = 0\n\n    logger.info(f\"[doc] processing {total_chunks} chunks for doc_id={work.doc_id}\")\n\n    for chunk_index, chunk_text in enumerate(work.chunks):\n        source_id = str(work.doc_id)\n        chunk_id = f\"c{chunk_index + 1}\"\n\n        logger.info(\n            f\"[doc] processing chunk {chunk_index + 1}/{total_chunks} for doc_id={work.doc_id}\"\n        )\n\n        try:\n            self.agent_orchestrator.process_chunk(\n                source_id, chunk_id, work.source_url, chunk_text\n            )\n            successful_chunks += 1\n            logger.info(\n                f\"[doc] chunk {chunk_index + 1}/{total_chunks} completed successfully for doc_id={work.doc_id}\"\n            )\n        except Exception as exc:\n            failed_chunks += 1\n            logger.error(\n                f\"[doc] chunk {chunk_index + 1}/{total_chunks} failed for doc_id={work.doc_id}: {exc}\",\n                exc_info=True,\n            )\n            # Continue processing other chunks even if one fails\n\n        # Optional Obsidian export\n        if Config.OBSIDIAN_EXPORT:\n            try:\n                self.paperless_connector.write_obsidian_note(\n                    work.doc, chunk_index, chunk_text\n                )\n                logger.debug(\n                    f\"[doc] obsidian export completed for chunk {chunk_index + 1} of doc_id={work.doc_id}\"\n                )\n            except Exception as exc:\n                logger.warning(\n                    f\"[doc] obsidian export failed for chunk {chunk_index + 1} of doc_id={work.doc_id}: {exc}\"\n                )\n\n    # Summary logging\n    logger.info(\n        f\"[doc] chunk processing completed for doc_id={work.doc_id}: {successful_chunks}/{total_chunks} successful, {failed_chunks} failed\"\n    )\n\n    if failed_chunks &gt; 0:\n        logger.warning(\n            f\"[doc] {failed_chunks} chunks failed processing for doc_id={work.doc_id} - some observations may be missing\"\n        )\n</code></pre>"},{"location":"api/processing/#processing.document_processor.DocumentProcessor.finalize_document","title":"<code>finalize_document(work)</code>","text":"<p>Finalize document processing and update state.</p> Source code in <code>importer/src/processing/document_processor.py</code> <pre><code>def finalize_document(self, work: DocumentWork):\n    \"\"\"Finalize document processing and update state.\"\"\"\n    self.state_manager.update_document_state(work.doc_id, work.text_hash)\n</code></pre>"},{"location":"api/processing/#processing.document_processor.DocumentProcessor.process_document","title":"<code>process_document(document)</code>","text":"<p>Process a single document through the complete pipeline.</p> Source code in <code>importer/src/processing/document_processor.py</code> <pre><code>def process_document(self, document: dict):\n    \"\"\"Process a single document through the complete pipeline.\"\"\"\n    work = self.prepare_document_work(document)\n    if not work:\n        return\n\n    self.run_downstream_steps(work)\n    self.finalize_document(work)\n</code></pre>"},{"location":"api/processing/#processing.document_processor.DocumentProcessor.run_main_process","title":"<code>run_main_process()</code>","text":"<p>Run the main document processing loop.</p> Source code in <code>importer/src/processing/document_processor.py</code> <pre><code>def run_main_process(self):\n    \"\"\"Run the main document processing loop.\"\"\"\n    try:\n        self.bootstrapper.bootstrap_all_services()\n        processed = 0\n\n        for document in self.paperless_connector.iter_documents():\n            # Check for shutdown signal from global state would go here if needed\n            try:\n                self.process_document(document)\n                processed += 1\n            except Exception as exc:\n                logger.error(f\"Failed to process document: {exc}\")\n\n        logger.info(f\"[run] completed; processed_docs={processed}\")\n\n    except Exception as exc:\n        logger.critical(f\"Fatal error in main process: {exc}\")\n        raise\n</code></pre>"},{"location":"api/processing/#processing.agent_orchestrator.AgentOrchestrator","title":"<code>AgentOrchestrator(neo4j_connector)</code>","text":"<p>Orchestrates AI agent processing with Neo4j knowledge graph.</p> Source code in <code>importer/src/processing/agent_orchestrator.py</code> <pre><code>def __init__(self, neo4j_connector: Neo4jMemoryConnector):\n    self.neo4j_connector = neo4j_connector\n</code></pre>"},{"location":"api/processing/#processing.agent_orchestrator.AgentOrchestrator.extract_relations_from_calls","title":"<code>extract_relations_from_calls(calls)</code>","text":"<p>Extract relations from tool calls.</p> Source code in <code>importer/src/processing/agent_orchestrator.py</code> <pre><code>def extract_relations_from_calls(\n    self, calls: List[Dict[str, Any]]\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Extract relations from tool calls.\"\"\"\n    relations: List[Dict[str, Any]] = []\n    for call in calls or []:\n        try:\n            if call.get(\"name\") != \"create_relations\":\n                continue\n            params = call.get(\"parameters\") or {}\n            rels = params.get(\"relations\") if isinstance(params, dict) else None\n            if isinstance(rels, list):\n                for rel in rels:\n                    if (\n                        isinstance(rel, dict)\n                        and rel.get(\"source\")\n                        and rel.get(\"relationType\")\n                        and rel.get(\"target\")\n                    ):\n                        relations.append(\n                            {\n                                \"source\": rel.get(\"source\"),\n                                \"relationType\": rel.get(\"relationType\"),\n                                \"target\": rel.get(\"target\"),\n                            }\n                        )\n        except (KeyError, TypeError, AttributeError):\n            continue\n    return relations\n</code></pre>"},{"location":"api/processing/#processing.agent_orchestrator.AgentOrchestrator.execute_tool_calls","title":"<code>execute_tool_calls(calls)</code>","text":"<p>Execute tool calls in the proper order.</p> Source code in <code>importer/src/processing/agent_orchestrator.py</code> <pre><code>def execute_tool_calls(self, calls: List[Dict[str, Any]]) -&gt; tuple[bool, List[str]]:\n    \"\"\"Execute tool calls in the proper order.\"\"\"\n    if not calls:\n        return False, []\n\n    wrote = False\n    touched: List[str] = []\n    ordered_calls = order_tool_calls(calls)\n    for idx, name, params in iter_valid_calls(ordered_calls):\n        success, names = invoke_and_collect(self, idx, name, params)\n        wrote = wrote or success\n        touched.extend(names)\n    return wrote, self._dedupe_preserve_order(touched)\n</code></pre>"},{"location":"api/processing/#processing.agent_orchestrator.AgentOrchestrator.ensure_evidence_links","title":"<code>ensure_evidence_links(entity_names, source_id, chunk_id, source_url)</code>","text":"<p>Ensure an evidence entity exists and is linked to all touched entities.</p> Source code in <code>importer/src/processing/agent_orchestrator.py</code> <pre><code>def ensure_evidence_links(\n    self, entity_names: List[str], source_id: str, chunk_id: str, source_url: str\n):\n    \"\"\"Ensure an evidence entity exists and is linked to all touched entities.\"\"\"\n    if not entity_names:\n        return\n\n    evidence_name = self._make_evidence_name(source_id, chunk_id)\n\n    try:\n        evidence_entity = self._build_evidence_entity(\n            source_id, chunk_id, source_url\n        )\n        self._invoke_tool(\n            \"create_entities\",\n            {\"entities\": [evidence_entity]},\n            log_prefix=\"evidence \",\n        )\n    except (ValueError, TypeError) as exc:\n        logger.warning(f\"[agent] evidence entity build error: {exc}\")\n    except RuntimeError as exc:\n        logger.warning(f\"[agent] evidence entity runtime error: {exc}\")\n\n    sources = [\n        name\n        for name in entity_names\n        if isinstance(name, str) and name and name != evidence_name\n    ]\n    if sources:\n        relations = [\n            {\"source\": name, \"relationType\": \"evidence\", \"target\": evidence_name}\n            for name in sources\n        ]\n        try:\n            self._invoke_tool(\n                \"create_relations\", {\"relations\": relations}, log_prefix=\"evidence \"\n            )\n            logger.info(\n                f\"[agent] linked {len(relations)} entities to evidence {evidence_name}\"\n            )\n        except (ValueError, TypeError) as exc:\n            logger.warning(f\"[agent] evidence relation param error: {exc}\")\n        except RuntimeError as exc:\n            logger.warning(f\"[agent] evidence relation runtime error: {exc}\")\n</code></pre>"},{"location":"api/processing/#processing.agent_orchestrator.AgentOrchestrator.process_chunk","title":"<code>process_chunk(source_id, chunk_id, source_url, text)</code>","text":"<p>Process a single chunk of text (modular pipeline).</p> Source code in <code>importer/src/processing/agent_orchestrator.py</code> <pre><code>def process_chunk(\n    self, source_id: str, chunk_id: str, source_url: str, text: str\n) -&gt; None:\n    \"\"\"Process a single chunk of text (modular pipeline).\"\"\"\n    logger.info(f\"[chunk] start doc={source_id} {chunk_id} len={len(text)}\")\n    self._log_chunk_start_preview(source_id, chunk_id, text)\n    wrote = False\n    touched: List[str] = []\n    relations_to_retry: List[Dict[str, Any]] = []\n    try:\n        wrote, touched, relations_to_retry, messages = self._stage_primary_prompt(\n            source_id, chunk_id, source_url, text\n        )\n        wrote, touched, relations_to_retry = self._stage_execute_ai_suggestions(\n            messages, wrote, touched, relations_to_retry, source_id, chunk_id\n        )\n        wrote, touched = self._stage_generate_calls_from_content(\n            messages, wrote, touched, source_id, chunk_id, source_url, text\n        )\n        wrote, touched = self._stage_force_minimal(\n            wrote, touched, source_id, chunk_id, source_url, text\n        )\n        self._stage_finalize(\n            wrote, touched, relations_to_retry, source_id, chunk_id, source_url\n        )\n    except Exception as exc:\n        logger.error(\n            f\"[chunk] FAILED doc={source_id} {chunk_id}: {exc}\", exc_info=True\n        )\n        with contextlib.suppress(Exception):\n            if not wrote and text and text.strip():\n                logger.info(\n                    f\"[chunk] attempting emergency fallback for failed doc={source_id} {chunk_id}\"\n                )\n                self._force_minimal_write(source_id, chunk_id, source_url, text)\n        raise\n</code></pre>"},{"location":"api/processing/#processing.state_manager.StateManager","title":"<code>StateManager(state_path=None)</code>","text":"<p>Manages processing state and idempotency checks.</p> Source code in <code>importer/src/processing/state_manager.py</code> <pre><code>def __init__(self, state_path: Path | None = None):\n    self.state_path = state_path or Config.STATE_PATH\n    self.state: Dict[str, Any] = {\"last_id\": 0, \"hashes\": {}}\n    self._load_state()\n</code></pre>"},{"location":"api/processing/#processing.state_manager.StateManager.save_state","title":"<code>save_state()</code>","text":"<p>Save the current state to file.</p> Source code in <code>importer/src/processing/state_manager.py</code> <pre><code>def save_state(self):\n    \"\"\"Save the current state to file.\"\"\"\n    with contextlib.suppress(Exception):\n        self.state_path.write_text(json.dumps(self.state), encoding=\"utf-8\")\n</code></pre>"},{"location":"api/processing/#processing.state_manager.StateManager.should_process_document","title":"<code>should_process_document(doc_id)</code>","text":"<p>Check if a document should be processed based on state.</p> Source code in <code>importer/src/processing/state_manager.py</code> <pre><code>def should_process_document(self, doc_id: int) -&gt; bool:\n    \"\"\"Check if a document should be processed based on state.\"\"\"\n    return True if Config.FORCE_REPROCESS else doc_id &gt; self.state.get(\"last_id\", 0)\n</code></pre>"},{"location":"api/processing/#processing.state_manager.StateManager.is_document_changed","title":"<code>is_document_changed(doc_id, text_hash)</code>","text":"<p>Check if document content has changed.</p> Source code in <code>importer/src/processing/state_manager.py</code> <pre><code>def is_document_changed(self, doc_id: int, text_hash: str) -&gt; bool:\n    \"\"\"Check if document content has changed.\"\"\"\n    return (\n        True\n        if Config.FORCE_REPROCESS\n        else text_hash != self.state.get(\"hashes\", {}).get(str(doc_id), \"\")\n    )\n</code></pre>"},{"location":"api/processing/#processing.state_manager.StateManager.update_document_state","title":"<code>update_document_state(doc_id, text_hash)</code>","text":"<p>Update state for processed document.</p> Source code in <code>importer/src/processing/state_manager.py</code> <pre><code>def update_document_state(self, doc_id: int, text_hash: str):\n    \"\"\"Update state for processed document.\"\"\"\n    self.state[\"hashes\"][str(doc_id)] = text_hash\n    self.state[\"last_id\"] = doc_id\n    self.save_state()\n    logger.info(f\"[state] saved id={doc_id} hash_prefix={text_hash[:8]}\")\n</code></pre>"},{"location":"api/processing/#processing.state_manager.StateManager.advance_last_id","title":"<code>advance_last_id(doc_id)</code>","text":"<p>Advance last processed ID (for skipped documents).</p> Source code in <code>importer/src/processing/state_manager.py</code> <pre><code>def advance_last_id(self, doc_id: int):\n    \"\"\"Advance last processed ID (for skipped documents).\"\"\"\n    self.state[\"last_id\"] = doc_id\n    self.save_state()\n    logger.info(f\"[state] advanced last_id={doc_id}\")\n</code></pre>"},{"location":"api/services/","title":"Services API","text":"<p>services bootstrap scheduler periodic job locking graceful shutdown</p>"},{"location":"api/services/#services.bootstrap.ServiceBootstrapper","title":"<code>ServiceBootstrapper</code>","text":"<p>Handles service availability checks and bootstrap process.</p>"},{"location":"api/services/#services.bootstrap.ServiceBootstrapper.wait_for_neo4j","title":"<code>wait_for_neo4j(host, port, timeout=240)</code>  <code>staticmethod</code>","text":"<p>Wait for Neo4j to become available.</p> Source code in <code>importer/src/services/bootstrap.py</code> <pre><code>@staticmethod\ndef wait_for_neo4j(host: str, port: int, timeout: int = 240):\n    \"\"\"Wait for Neo4j to become available.\"\"\"\n    logger.info(f\"[bootstrap] Waiting for Neo4j at {host}:{port} ...\")\n    start_time = time.time()\n\n    while time.time() - start_time &lt; timeout:\n        try:\n            with socket.create_connection((host, port), timeout=2):\n                logger.info(\"[bootstrap] Neo4j available.\")\n                return\n        except OSError:\n            time.sleep(2)\n\n    raise RuntimeError(\"Neo4j not available.\")\n</code></pre>"},{"location":"api/services/#services.bootstrap.ServiceBootstrapper.wait_for_http_service","title":"<code>wait_for_http_service(url, timeout=240)</code>  <code>staticmethod</code>","text":"<p>Wait for HTTP service to become available.</p> Source code in <code>importer/src/services/bootstrap.py</code> <pre><code>@staticmethod\ndef wait_for_http_service(url: str, timeout: int = 240):\n    \"\"\"Wait for HTTP service to become available.\"\"\"\n    logger.info(f\"[bootstrap] Waiting for HTTP service: {url}\")\n    start_time = time.time()\n\n    while time.time() - start_time &lt; timeout:\n        with contextlib.suppress(Exception):\n            with httpx.Client(timeout=5) as client:\n                response = client.get(url)\n                if 200 &lt;= response.status_code &lt; 500:\n                    logger.info(f\"[bootstrap] Available: {url}\")\n                    return\n        time.sleep(2)\n\n    raise RuntimeError(f\"Service not available: {url}\")\n</code></pre>"},{"location":"api/services/#services.bootstrap.ServiceBootstrapper.resolve_neo4j_host_port","title":"<code>resolve_neo4j_host_port()</code>  <code>classmethod</code>","text":"<p>Resolve Neo4j host and port from configuration.</p> Source code in <code>importer/src/services/bootstrap.py</code> <pre><code>@classmethod\ndef resolve_neo4j_host_port(cls) -&gt; tuple[str, int]:\n    \"\"\"Resolve Neo4j host and port from configuration.\"\"\"\n    if Config.NEO4J_URL:\n        if host_port := Neo4jMemoryConnector.neo4j_host_port_from_url(\n            Config.NEO4J_URL\n        ):\n            return host_port\n    return Config.NEO4J_HOST, Config.NEO4J_PORT\n</code></pre>"},{"location":"api/services/#services.bootstrap.ServiceBootstrapper.bootstrap_all_services","title":"<code>bootstrap_all_services()</code>  <code>classmethod</code>","text":"<p>Bootstrap all required services.</p> Source code in <code>importer/src/services/bootstrap.py</code> <pre><code>@classmethod\ndef bootstrap_all_services(cls):\n    \"\"\"Bootstrap all required services.\"\"\"\n    cls.wait_for_http_service(Config.PAPERLESS_URL, timeout=600)\n    host, port = cls.resolve_neo4j_host_port()\n    cls.wait_for_neo4j(host, port, timeout=600)\n    cls.wait_for_http_service(f\"{Config.OLLAMA_URL}/api/tags\", timeout=600)\n    PaperlessConnector.get_headers()  # Validate token\n    logger.info(\"[bootstrap] Services ready.\")\n</code></pre>"},{"location":"api/services/#services.scheduler.SchedulerCoordinator","title":"<code>SchedulerCoordinator(processor)</code>","text":"<p>Coordinates scheduled execution and handles concurrency.</p> Source code in <code>importer/src/services/scheduler.py</code> <pre><code>def __init__(self, processor):\n    self.document_processor = processor\n    self.run_lock = threading.Lock()\n    self.stop_event = threading.Event()\n</code></pre>"},{"location":"api/services/#services.scheduler.SchedulerCoordinator.run_scheduled_job","title":"<code>run_scheduled_job()</code>","text":"<p>Execute the main processing job if not already running.</p> Source code in <code>importer/src/services/scheduler.py</code> <pre><code>def run_scheduled_job(self):\n    \"\"\"Execute the main processing job if not already running.\"\"\"\n    if self.stop_event.is_set():\n        return\n\n    acquired = self.run_lock.acquire(blocking=False)\n    if not acquired:\n        logger.warning(\n            \"Previous run still in progress; skipping this schedule tick.\"\n        )\n        return\n\n    try:\n        if not self.stop_event.is_set():\n            self.document_processor.run_main_process()\n    finally:\n        self.run_lock.release()\n</code></pre>"},{"location":"api/services/#services.scheduler.SchedulerCoordinator.run_initial_process","title":"<code>run_initial_process()</code>","text":"<p>Run initial processing if possible.</p> Source code in <code>importer/src/services/scheduler.py</code> <pre><code>def run_initial_process(self):\n    \"\"\"Run initial processing if possible.\"\"\"\n    if self.stop_event.is_set():\n        logger.info(\"Shutdown requested before run; skipping main().\")\n        return\n\n    if self.run_lock.acquire(blocking=False):\n        try:\n            self.document_processor.run_main_process()\n        finally:\n            self.run_lock.release()\n    else:\n        logger.warning(\"Importer already running at startup; initial run skipped.\")\n</code></pre>"},{"location":"api/services/#services.scheduler.SchedulerCoordinator.start_scheduler","title":"<code>start_scheduler()</code>","text":"<p>Start the scheduled execution loop.</p> Source code in <code>importer/src/services/scheduler.py</code> <pre><code>def start_scheduler(self):\n    \"\"\"Start the scheduled execution loop.\"\"\"\n    schedule.every(Config.SCHEDULE_TIME).minutes.do(self.run_scheduled_job)\n\n    while not self.stop_event.is_set():\n        schedule.run_pending()\n        time.sleep(1)\n</code></pre>"},{"location":"api/services/#services.scheduler.SchedulerCoordinator.request_stop","title":"<code>request_stop()</code>","text":"<p>Request a graceful shutdown.</p> Source code in <code>importer/src/services/scheduler.py</code> <pre><code>def request_stop(self):\n    \"\"\"Request a graceful shutdown.\"\"\"\n    self.stop_event.set()\n</code></pre>"},{"location":"api/utils/","title":"Utils API","text":"<p>utilities text parsing normalization tool calls extraction transformer helpers</p>"},{"location":"api/utils/#utils.text_utils.TextUtils","title":"<code>TextUtils</code>","text":"<p>Utility class for text processing operations.</p>"},{"location":"api/utils/#utils.text_utils.TextUtils.strip_code_fences","title":"<code>strip_code_fences(text)</code>  <code>staticmethod</code>","text":"<p>Remove code fences from text.</p> Source code in <code>importer/src/utils/text_utils.py</code> <pre><code>@staticmethod\ndef strip_code_fences(text: str) -&gt; str:\n    \"\"\"Remove code fences from text.\"\"\"\n    if not text:\n        return text\n    return re.sub(r\"```[a-zA-Z]*\\s*\\r?\\n|```\", \"\", text, flags=re.MULTILINE)\n</code></pre>"},{"location":"api/utils/#utils.text_utils.TextUtils.truncate_text","title":"<code>truncate_text(text, limit)</code>  <code>staticmethod</code>","text":"<p>Truncate text to specified limit with indicator.</p> Source code in <code>importer/src/utils/text_utils.py</code> <pre><code>@staticmethod\ndef truncate_text(text: Any, limit: int) -&gt; str:\n    \"\"\"Truncate text to specified limit with indicator.\"\"\"\n    try:\n        s = text if isinstance(text, str) else json.dumps(text, ensure_ascii=False)\n    except Exception:\n        s = str(text)\n    if limit &lt;= 0 or len(s) &lt;= limit:\n        return s\n    tail = len(s) - limit\n    return f\"{s[:limit]}... [truncated {tail} chars]\"\n</code></pre>"},{"location":"api/utils/#utils.text_utils.TextUtils.chunk_text","title":"<code>chunk_text(text, max_chars=None)</code>  <code>staticmethod</code>","text":"<p>Split text into chunks of a specified size.</p> Source code in <code>importer/src/utils/text_utils.py</code> <pre><code>@staticmethod\ndef chunk_text(text: str, max_chars: Optional[int] = None) -&gt; List[str]:\n    \"\"\"Split text into chunks of a specified size.\"\"\"\n    if max_chars is None:\n        max_chars = Config.CHUNK_SIZE\n    return (\n        [text[i : i + max_chars] for i in range(0, len(text), max_chars)]\n        if text\n        else [\"\"]\n    )\n</code></pre>"},{"location":"api/utils/#utils.json_parser.JSONParser","title":"<code>JSONParser</code>","text":"<p>JSON parsing utilities for extracting structured data.</p>"},{"location":"api/utils/#utils.json_parser.JSONParser.is_valid_tool_call","title":"<code>is_valid_tool_call(obj)</code>  <code>staticmethod</code>","text":"<p>Check if an object is a valid tool call with name and parameters.</p> Source code in <code>importer/src/utils/json_parser.py</code> <pre><code>@staticmethod\ndef is_valid_tool_call(obj: Any) -&gt; bool:\n    \"\"\"Check if an object is a valid tool call with name and parameters.\"\"\"\n    return isinstance(obj, dict) and \"name\" in obj and \"parameters\" in obj\n</code></pre>"},{"location":"api/utils/#utils.json_parser.JSONParser.extract_json_object_at_position","title":"<code>extract_json_object_at_position(text, start_pos)</code>  <code>staticmethod</code>","text":"<p>Extract a complete JSON object starting at the given position.</p> Source code in <code>importer/src/utils/json_parser.py</code> <pre><code>@staticmethod\ndef extract_json_object_at_position(text: str, start_pos: int) -&gt; Optional[tuple[str, int]]:\n    \"\"\"Extract a complete JSON object starting at the given position.\"\"\"\n    depth = 0\n    i = start_pos + 1  # Skip opening brace\n    in_string = False\n    escaped = False\n\n    while i &lt; len(text):\n        char = text[i]\n\n        if in_string:\n            if escaped:\n                escaped = False\n            elif char == '\\\\':\n                escaped = True\n            elif char == '\"':\n                in_string = False\n        elif char == '{':\n            depth += 1\n        elif char == '}':\n            if depth == 0:\n                return text[start_pos:i+1], i\n            depth -= 1\n        elif char == '\"':\n            in_string = True\n        i += 1\n\n    return None\n</code></pre>"},{"location":"api/utils/#utils.json_parser.JSONParser.extract_json_array_at_position","title":"<code>extract_json_array_at_position(text, start_pos)</code>  <code>staticmethod</code>","text":"<p>Extract a complete JSON array starting at the given position.</p> Source code in <code>importer/src/utils/json_parser.py</code> <pre><code>@staticmethod\ndef extract_json_array_at_position(text: str, start_pos: int) -&gt; Optional[str]:\n    \"\"\"Extract a complete JSON array starting at the given position.\"\"\"\n    depth = 0\n    i = start_pos + 1  # Skip opening bracket\n    in_string = False\n    escaped = False\n\n    while i &lt; len(text):\n        char = text[i]\n\n        if in_string:\n            if escaped:\n                escaped = False\n            elif char == '\\\\':\n                escaped = True\n            elif char == '\"':\n                in_string = False\n        elif char == '[':\n            depth += 1\n        elif char == ']':\n            if depth == 0:\n                return text[start_pos:i+1]\n            depth -= 1\n        elif char == '\"':\n            in_string = True\n        i += 1\n\n    return None\n</code></pre>"},{"location":"api/utils/#utils.json_parser.JSONParser.iter_json_objects","title":"<code>iter_json_objects(text)</code>  <code>classmethod</code>","text":"<p>Yield JSON objects from string using balanced brace scanning.</p> Source code in <code>importer/src/utils/json_parser.py</code> <pre><code>@classmethod\ndef iter_json_objects(cls, text: str) -&gt; Iterator[str]:\n    \"\"\"Yield JSON objects from string using balanced brace scanning.\"\"\"\n    i = 0\n    n = len(text)\n\n    while i &lt; n:\n        if text[i] == '{':\n            if json_obj := cls.extract_json_object_at_position(text, i):\n                yield json_obj[0]\n                i = json_obj[1] + 1\n            else:\n                i += 1\n        else:\n            i += 1\n</code></pre>"},{"location":"api/utils/#utils.json_parser.JSONParser.find_top_level_array","title":"<code>find_top_level_array(text)</code>  <code>classmethod</code>","text":"<p>Find and extract the first top-level JSON array in the string.</p> Source code in <code>importer/src/utils/json_parser.py</code> <pre><code>@classmethod\ndef find_top_level_array(cls, text: str) -&gt; Optional[str]:\n    \"\"\"Find and extract the first top-level JSON array in the string.\"\"\"\n    for i in range(len(text)):\n        if text[i] == '[':\n            if array_content := cls.extract_json_array_at_position(text, i):\n                return array_content\n    return None\n</code></pre>"},{"location":"api/utils/#utils.tool_call_extractor.ToolCallExtractor","title":"<code>ToolCallExtractor</code>","text":"<p>Simplified extractor using high-performance json_utils.</p> <p>We expect model output to contain either a JSON array or object describing tool calls. Only a single pass extraction is attempted to reduce complexity and maintenance burden.</p>"},{"location":"api/utils/#utils.tool_call_normalizer.ToolCallNormalizer","title":"<code>ToolCallNormalizer</code>","text":"<p>Normalizes tool call parameters to ensure consistency.</p>"},{"location":"api/utils/#utils.tool_call_normalizer.ToolCallNormalizer.extract_name","title":"<code>extract_name(value)</code>  <code>staticmethod</code>","text":"<p>Extract name from dict or return value as-is.</p> Source code in <code>importer/src/utils/tool_call_normalizer.py</code> <pre><code>@staticmethod\ndef extract_name(value: Any) -&gt; Any:\n    \"\"\"Extract name from dict or return value as-is.\"\"\"\n    return value.get(\"name\") if isinstance(value, dict) and \"name\" in value else value\n</code></pre>"},{"location":"api/utils/#utils.tool_call_normalizer.ToolCallNormalizer.extract_relation_entity_names","title":"<code>extract_relation_entity_names(relation)</code>  <code>classmethod</code>","text":"<p>Extract entity names from a relation dict (source and target).</p> Source code in <code>importer/src/utils/tool_call_normalizer.py</code> <pre><code>@classmethod\ndef extract_relation_entity_names(cls, relation: Dict[str, Any]) -&gt; List[str]:\n    \"\"\"Extract entity names from a relation dict (source and target).\"\"\"\n    names = []\n    if isinstance(relation, dict):\n        source = relation.get(\"source\")\n        target = relation.get(\"target\")\n\n        if isinstance(source, dict):\n            source = source.get(\"name\")\n        if isinstance(target, dict):\n            target = target.get(\"name\")\n\n        if isinstance(source, str):\n            names.append(source)\n        if isinstance(target, str):\n            names.append(target)\n    return names\n</code></pre>"},{"location":"api/utils/#utils.tool_call_normalizer.ToolCallNormalizer.normalize_single_relation","title":"<code>normalize_single_relation(relation)</code>  <code>classmethod</code>","text":"<p>Normalize a single relation dictionary.</p> Source code in <code>importer/src/utils/tool_call_normalizer.py</code> <pre><code>@classmethod\ndef normalize_single_relation(cls, relation: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Normalize a single relation dictionary.\"\"\"\n    normalized = dict(relation)\n\n    # Handle field name variations\n    if \"subject\" in normalized and \"source\" not in normalized:\n        normalized[\"source\"] = cls.extract_name(normalized.pop(\"subject\"))\n    if \"object\" in normalized and \"target\" not in normalized:\n        normalized[\"target\"] = cls.extract_name(normalized.pop(\"object\"))\n    if \"predicate\" in normalized and \"relationType\" not in normalized:\n        normalized[\"relationType\"] = normalized.pop(\"predicate\")\n\n    normalized[\"source\"] = cls.extract_name(normalized.get(\"source\"))\n    normalized[\"target\"] = cls.extract_name(normalized.get(\"target\"))\n    return normalized\n</code></pre>"},{"location":"api/utils/#utils.tool_call_normalizer.ToolCallNormalizer.normalize_relations_params","title":"<code>normalize_relations_params(params)</code>  <code>classmethod</code>","text":"<p>Normalize parameters for create_relations tool.</p> Source code in <code>importer/src/utils/tool_call_normalizer.py</code> <pre><code>@classmethod\ndef normalize_relations_params(cls, params: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Normalize parameters for create_relations tool.\"\"\"\n    relations = params.get(\"relations\")\n\n    if not relations:\n        # Try to build from individual fields\n        maybe = {k: params.get(k) for k in (\"source\", \"predicate\", \"relationType\", \"target\", \"when\", \"evidence\", \"confidence\", \"sourceId\", \"chunkId\", \"sourceUrl\") if k in params}\n        if any(maybe.values()):\n            relations = [maybe]\n\n    normalized_relations = [\n        cls.normalize_single_relation(r)\n        for r in relations or []\n        if isinstance(r, dict)\n    ]\n\n    result = {k: v for k, v in params.items() if k != \"relations\"}\n    result[\"relations\"] = normalized_relations\n    return result\n</code></pre>"},{"location":"api/utils/#utils.tool_call_normalizer.ToolCallNormalizer.normalize_observation","title":"<code>normalize_observation(observation)</code>  <code>classmethod</code>","text":"<p>Normalize a single observation dictionary.</p> Source code in <code>importer/src/utils/tool_call_normalizer.py</code> <pre><code>@classmethod\ndef normalize_observation(cls, observation: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Normalize a single observation dictionary.\"\"\"\n    normalized = dict(observation)\n\n    # Normalize entity name field\n    entity_name_mappings = [\n        (\"entity\", \"entityName\"),\n        (\"entity_name\", \"entityName\"),\n        (\"name\", \"entityName\")\n    ]\n\n    for old_key, new_key in entity_name_mappings:\n        if old_key in normalized and new_key not in normalized:\n            normalized[new_key] = cls.extract_name(normalized.pop(old_key))\n\n    # Normalize observations field\n    if \"observations\" not in normalized or not isinstance(normalized.get(\"observations\"), list):\n        if \"text\" in normalized and normalized[\"text\"]:\n            normalized[\"observations\"] = [str(normalized.pop(\"text\"))]\n        else:\n            normalized[\"observations\"] = []\n\n    return normalized\n</code></pre>"},{"location":"api/utils/#utils.tool_call_normalizer.ToolCallNormalizer.extract_observations_list","title":"<code>extract_observations_list(params)</code>  <code>classmethod</code>","text":"<p>Extract the observation list from parameters.</p> Source code in <code>importer/src/utils/tool_call_normalizer.py</code> <pre><code>@classmethod\ndef extract_observations_list(cls, params: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\n    \"\"\"Extract the observation list from parameters.\"\"\"\n    if observations := params.get(\"observations\"):\n        return observations\n\n    if \"observation\" in params and isinstance(params[\"observation\"], dict):\n        return [params[\"observation\"]]\n\n    if any(k in params for k in (\"entityName\", \"text\", \"observations\")):\n        observations_list = cls.build_observations_list(params)\n        return [{\"entityName\": params.get(\"entityName\"), \"observations\": observations_list}]\n\n    return []\n</code></pre>"},{"location":"api/utils/#utils.tool_call_normalizer.ToolCallNormalizer.build_observations_list","title":"<code>build_observations_list(params)</code>  <code>classmethod</code>","text":"<p>Build the observation list from parameters.</p> Source code in <code>importer/src/utils/tool_call_normalizer.py</code> <pre><code>@classmethod\ndef build_observations_list(cls, params: Dict[str, Any]) -&gt; List[str]:\n    \"\"\"Build the observation list from parameters.\"\"\"\n    if isinstance(params.get(\"observations\"), list):\n        return [str(x) for x in params[\"observations\"]]\n    elif \"text\" in params and params[\"text\"]:\n        return [str(params[\"text\"])]\n    return []\n</code></pre>"},{"location":"api/utils/#utils.tool_call_normalizer.ToolCallNormalizer.normalize_observations_params","title":"<code>normalize_observations_params(params)</code>  <code>classmethod</code>","text":"<p>Normalize parameters for the add_observations tool.</p> Source code in <code>importer/src/utils/tool_call_normalizer.py</code> <pre><code>@classmethod\ndef normalize_observations_params(cls, params: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Normalize parameters for the add_observations tool.\"\"\"\n    observations = cls.extract_observations_list(params)\n    normalized_observations = [cls.normalize_observation(o) for o in observations or [] if isinstance(o, dict)]\n\n    result = {k: v for k, v in params.items() if k not in {\"observations\", \"observation\", \"text\"}}\n    result[\"observations\"] = normalized_observations\n    return result\n</code></pre>"},{"location":"api/utils/#utils.tool_call_normalizer.ToolCallNormalizer.normalize_single_entity","title":"<code>normalize_single_entity(entity)</code>  <code>classmethod</code>","text":"<p>Normalize a single entity dictionary.</p> Source code in <code>importer/src/utils/tool_call_normalizer.py</code> <pre><code>@classmethod\ndef normalize_single_entity(cls, entity: Dict[str, Any]) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Normalize a single entity dictionary.\"\"\"\n    normalized = {k: v for k, v in entity.items() if k in {\"name\", \"type\", \"observations\"}}\n\n    if \"name\" not in normalized:\n        return None\n\n    if \"observations\" not in normalized or not isinstance(normalized.get(\"observations\"), list):\n        normalized[\"observations\"] = []\n\n    return normalized\n</code></pre>"},{"location":"api/utils/#utils.tool_call_normalizer.ToolCallNormalizer.extract_entities_from_params","title":"<code>extract_entities_from_params(params)</code>  <code>classmethod</code>","text":"<p>Extract entities list from various parameter formats.</p> Source code in <code>importer/src/utils/tool_call_normalizer.py</code> <pre><code>@classmethod\ndef extract_entities_from_params(cls, params: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\n    \"\"\"Extract entities list from various parameter formats.\"\"\"\n    if entities := params.get(\"entities\"):\n        return entities\n\n    if \"entity\" in params and isinstance(params[\"entity\"], dict):\n        return [params[\"entity\"]]\n\n    if any(k in params for k in (\"name\", \"type\")):\n        entity = {k: params.get(k) for k in (\"name\", \"type\") if k in params}\n        return [entity]\n\n    if cls.has_valid_observations(params):\n        first_observation = params[\"observations\"][0]\n        entity = {\n            \"name\": first_observation[\"entityName\"], \n            \"type\": params.get(\"type\", \"Thing\")\n        }\n        return [entity]\n\n    return []\n</code></pre>"},{"location":"api/utils/#utils.tool_call_normalizer.ToolCallNormalizer.has_valid_observations","title":"<code>has_valid_observations(params)</code>  <code>classmethod</code>","text":"<p>Check if parameters contain valid observations with an entity name.</p> Source code in <code>importer/src/utils/tool_call_normalizer.py</code> <pre><code>@classmethod\ndef has_valid_observations(cls, params: Dict[str, Any]) -&gt; bool:\n    \"\"\"Check if parameters contain valid observations with an entity name.\"\"\"\n    observations = params.get(\"observations\")\n    if not isinstance(observations, list) or not observations:\n        return False\n\n    first_observation = observations[0]\n    return isinstance(first_observation, dict) and \"entityName\" in first_observation\n</code></pre>"},{"location":"api/utils/#utils.tool_call_normalizer.ToolCallNormalizer.normalize_entities_params","title":"<code>normalize_entities_params(params)</code>  <code>classmethod</code>","text":"<p>Normalize parameters for the create_entities tool.</p> Source code in <code>importer/src/utils/tool_call_normalizer.py</code> <pre><code>@classmethod\ndef normalize_entities_params(cls, params: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Normalize parameters for the create_entities tool.\"\"\"\n    entities = cls.extract_entities_from_params(params)\n    normalized_entities = [\n        normalized_entity \n        for entity in entities or []\n        if isinstance(entity, dict) and (normalized_entity := cls.normalize_single_entity(entity))\n    ]\n\n    result = {k: v for k, v in params.items() if k != \"entities\"}\n    result[\"entities\"] = normalized_entities\n    return result\n</code></pre>"},{"location":"api/utils/#utils.tool_call_normalizer.ToolCallNormalizer.normalize_params","title":"<code>normalize_params(tool_name, params)</code>  <code>classmethod</code>","text":"<p>Normalize parameters for different tools.</p> Source code in <code>importer/src/utils/tool_call_normalizer.py</code> <pre><code>@classmethod\ndef normalize_params(cls, tool_name: str, params: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Normalize parameters for different tools.\"\"\"\n    normalizers = {\n        \"create_relations\": cls.normalize_relations_params,\n        \"add_observations\": cls.normalize_observations_params,\n        \"create_entities\": cls.normalize_entities_params,\n    }\n\n    normalizer = normalizers.get(tool_name)\n    return normalizer(params) if normalizer else params\n</code></pre>"}]}